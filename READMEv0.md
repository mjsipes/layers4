# Layers ‚ùÑÔ∏èüéø

**AI‚Äëintegrated wardrobe. So you never dress too hot or too cold.**

## Try It Out

**Live:** [https://layers4.vercel.app/](https://layers4.vercel.app/)

Here‚Äôs a quick sequence to see the app in action:

### Demo Flow (Recommended)

1. Ask: "What is my location?"
2. Ask: "What is the weather?"
3. Ask: "What should I wear?" *(notice limited recommendations)*
4. Tell the AI: "I'm wearing \[your outfit] and I feel \[too hot/cold/perfect]"
5. Add layers: "I have \[list items] in my wardrobe"
6. Ask again: "What should I wear?" *(see improved recommendations)*

*Or try the pre‚Äëpopulated demo account: **[mjsipes@gmail.com](mailto:mjsipes@gmail.com)** / **123456***

---

## The Journey Behind the Tech

One day last winter I was skiing in Lake Tahoe. As someone who grew up skiing maybe 10 days a year, I never dressed right for the mountains. Twenty degrees and thirty degrees both sound like "freezing" when you‚Äôre used to 60s and 70s, but that ten‚Äëdegree difference can be the difference between a perfect day and being too hot or too cold.

Then I had a thought: build an app that connects an LLM to a database of **Layers (articles of clothing)** and **Logs**, and see if it can give great recommendations based simply on prior days‚Äô answers to:

* What did you wear today?
* Where are you?
* What day is it?
* Were you too hot, too cold, or perfect?

This was the start of **Layers**.

A summer ago I built a RAG solution to help an AI model give strong answers to support questions for RingCentral article writers. The common thread across projects: it‚Äôs now standard in CS to identify data that pairs well with LLMs so they can produce better output.

I can use Layers throughout the winter and measure accuracy. With as little as three previous days of experience, I believe it will work. I skied 25 days last season, so I‚Äôll have \~30 days of great context to evaluate.

## Why LLM + Weather Alone Isn‚Äôt Enough

**If I ask ChatGPT:**

**User:** What is the weather?

**Model behavior:** it will web‚Äësearch and return the weather.

**Response example:**

> It‚Äôs clear and mild in Lake Tahoe right now, around 70 ¬∞F (21 ¬∞C). Tonight will cool into the low 50s ¬∞F (10‚Äì12 ¬∞C) with clear skies, and tomorrow morning will start sunny in the mid‚Äë50s.

**If I ask ChatGPT:**

**User:** What should I wear?

**Model behavior:** it will web‚Äësearch and return the weather *and* reason about what you should wear, then give a detailed recommendation.

**Response example:**

> Tonight in Lake Tahoe it will be clear and cool, dipping into the low 50s ¬∞F (10‚Äì12 ¬∞C). A light jacket or sweater, long pants, and maybe a warmer layer if you‚Äôll be out late will keep you comfortable.

**The problem:** it doesn‚Äôt know your wardrobe or your personal comfort profile.

I‚Äôve gone to Giants games with friends wearing a hoodie and still felt freezing. I also have a friend who could go in a t‚Äëshirt and be fine‚Äîand might even call the weather ‚Äúnice.‚Äù Everyone is different. One recommendation won‚Äôt fit everybody. So the prompt ‚Äúwhat should I wear?‚Äù shouldn‚Äôt be the same for every user. It must include context about **you**: what layers you own, examples of what you wore on certain days, what the weather was like, and whether you were too hot or too cold.

## My Hypothesis (Prompts & Tools)

If you create a prompt like the following, you‚Äôll get much more accurate recommendations.

**System (example):**

```
Context:
User owns the following layers (articles of clothing):
- USC Hoodie
- Tommy Trojan pajama pants
- "Fight On" bracelet

Here are relevant logs for similar conditions:
- 2025-01-18 ‚Äî Lake Tahoe, CA ‚Äî High 28¬∞F / Low 12¬∞F, wind 8 mph NNE. Wore: base layer top + snowboard shell, USC Hoodie midlayer, ski pants, wool socks, gloves, beanie. Feedback: felt slightly cold on chairlifts; would add thicker midlayer or neck gaiter next time.

User:
I'm in Lake Tahoe today. What should I wear?
```

Or:

**System (example):**

```
The user will often ask you about weather recommendations: "What should I wear today?"
If a user uses this app, returning logs and layers will provide lots of context about the user‚Äôs previous behavior. Make those tool calls and reason over the results before responding. If the user has no logs to reason over, make your best guess. If the user has no layers to reason over‚Äîor not enough to make an effective outfit‚Äîask whether they own the needed layers (and if not, recommend a purchase).

Continually, the user is encouraged to share information about their wardrobe and previous experiences; you can log those with add_log and add_layer. If a user brings up logs in conversation, do a quick wardrobe search with search_layers; we recommend search_layers first because it‚Äôs semantic and returns relevant results, but if you don‚Äôt get results it‚Äôs okay to call get_layers.

Tools:
- get_user_logs
- get_user_layers
- search_layers
- get_layers
- add_log
- add_layer

User:
I am in Lake Tahoe. What should I wear today?
```

---

## Architecture Decisions & Exploration

The core loop is two simple interactions‚Äî**‚ÄúWhat did you wear today?‚Äù** and **‚ÄúWhat should I wear today?‚Äù** The project unfolded in that order:

## **‚ÄúWhat did you wear today?‚Äù ‚Äî Collecting data via chat**
   I don‚Äôt want users to manually enter all their logs and layers. Instead, the AI should add logs and layers it learns from natural conversation.

**Inputs I want to support:**

1. User types in natural language
2. User types in by hand (structured entry)
3. User uploads a photo

This led to the first version of the AI agent‚Äîand to a key architectural choice. To connect the AI to my database, I needed a safe bridge so the model could call a small set of functions (e.g., fetch logs, add layers, write feedback) instead of touching the DB directly. There are two modern ways to expose those functions: **Tool Calling** and the newer **Model Context Protocol (MCP)**.

### Big Decisions: MCP vs Tool Calling

At first I framed the choice as ‚Äúfuture USB‚Äù vs ‚Äúold and janky.‚Äù After two weeks exploring, I learned it‚Äôs really **simple & lightweight** vs **spanning many chat interfaces**. For this project, **tool calling** is totally fine.

* **I spent two weeks trying to make it MCP.** I dove deep into the emerging MCP ecosystem:

  * How to structure MCP servers
  * Deploying to Vercel and Cloudflare Workers
  * OAuth authentication for MCP
  * Using the MCP inspector tools

  **Documentation I explored:**

  * Model Context Protocol: [https://modelcontextprotocol.io/docs/getting-started/intro](https://modelcontextprotocol.io/docs/getting-started/intro)
  * OpenAI‚Äôs MCP integration guides: [https://platform.openai.com/docs/guides/tools-connectors-mcp](https://platform.openai.com/docs/guides/tools-connectors-mcp)
  * Vercel‚Äôs MCP server deployment: [https://vercel.com/docs/mcp/deploy-mcp-servers-to-vercel](https://vercel.com/docs/mcp/deploy-mcp-servers-to-vercel)
  * Cloudflare‚Äôs remote MCP server guides: [https://developers.cloudflare.com/agents/guides/remote-mcp-server/](https://developers.cloudflare.com/agents/guides/remote-mcp-server/)

* **Tool calling:** I evaluated OpenAI API tool/function calling and the Vercel AI SDK tool calling:

  * OpenAI function calling: [https://platform.openai.com/docs/guides/function-calling](https://platform.openai.com/docs/guides/function-calling)
  * Vercel AI SDK tools: [https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling](https://ai-sdk.dev/docs/ai-sdk-core/tools-and-tool-calling)

**Outcome:** I used **tool calling** for Layers.

## **‚ÄúWhat should I wear today?‚Äù ‚Äî Presenting results to the user**
   Once the plumbing worked, I asked: *Should the app present results purely as chat/Markdown, as a full UI, or both?* That became a major focus of the summer. Initially the agent replied with Markdown. Would a fully fledged UI be better? I explored **AI**, **UI**, **both**, and **mixed**.

---

###  Experiment 1: Dual AI UI and Traditional UI

Layers uses a split‚Äëscreen, resizable layout where both the traditional UI and the AI interface can perform **identical operations** on the database. Both paths can do the same things, and the AI can **control the UI** through dynamic cards.

* The AI can dynamically surface components‚Äîdisplaying specific logs, layers, or immediately showing newly added items.
* When the AI adds layers, it can immediately display them for user review.
* Both the traditional interface and AI chat handle the same CRUD operations (select, insert, update, delete) on **Logs** and **Layers**.

---

### Experiment 2: Keyword Search vs Semantic Search vs Hybrid Search

The shadcn **Data Table** component caught my eye this summer. It made me ask: *When is it appropriate to graduate from a simple table to a full data table?* (A ‚Äúdata table‚Äù = table + select/sort/filter.)

**References:**

* Data Table: [https://ui.shadcn.com/docs/components/data-table](https://ui.shadcn.com/docs/components/data-table)
* Table: [https://ui.shadcn.com/docs/components/table](https://ui.shadcn.com/docs/components/table)

Under the hood, the data table uses **@tanstack/table**‚Äîan NPM package I‚Äôve been curious about. (TanStack also offers a router, which I looked at while comparing React Router, TanStack Router, and Next.js file‚Äësystem routing.)

The data table‚Äôs **search filter** is great: it‚Äôs a **keyword search on the client in JavaScript**‚Äîwicked fast and snappy.

I wondered if I could enhance it with **semantic search**. What if you search for layers but misspell them? What if you search by a log‚Äôs description but don‚Äôt remember exact phrasing? Or you type ‚Äúcold‚Äù and want all cold days even if ‚Äúcold‚Äù isn‚Äôt literally in the log?

So I implemented it. On **every keystroke** in the search bar, the client sent a request to a **Supabase Edge Function** which would vector‚Äëembed the query and match it to logs/layers in the **embedding columns** of those tables. I vector‚Äëembedded **all** logs and layers using Supabase‚Äôs guide: [https://supabase.com/docs/guides/ai/semantic-search](https://supabase.com/docs/guides/ai/semantic-search)

I built a **hybrid search**:

* Keyword across multiple fields (for logs: date, place, outfits worn, description)
* Semantic via vector embeddings for fuzzy matching

**Problem I‚Äôm still looking for help on:** In RAG, there‚Äôs ‚Äúart‚Äù in how you embed data. I didn‚Äôt spend a lot of time tuning this. What‚Äôs the best way to implement **hybrid search** here? Do we need hybrid search to **sort by values**, or is there a way to **embed hard knowledge** like temperatures so queries like ‚Äú50‚Äù or ‚Äúfifty‚Äù match effectively?

**The brutal reality:** The advantage of pure **client‚Äëside keyword search** (instant results on every keystroke) outweighed server‚Äëside hybrid search (always triggers a spinner, even if brief). My data isn‚Äôt that complex. Keyword search across **all** entries (date, location, outfits, description) feels smart *and* fast.

Worth noting: my app loads all of a user‚Äôs logs and layers to the client. In apps like Gmail or Google where the client doesn‚Äôt load everything up front, hybrid/server search makes more sense because keyword search must go to the server anyway‚Äîso combining AI + keyword search is appealing there.

---


## Ideas & Next Steps

### AI‚ÄëPowered Multiselect

When logging what you wore, the multiselect uses keyword search‚Äîbut it can‚Äôt **add new layers** from that input. If the placeholder is ‚ÄúWhat did you wear today?‚Äù and a user types ‚ÄúI wore a blue shirt,‚Äù it won‚Äôt find anything if there‚Äôs no keyword match.

The log can contain an **array of layers**, and I need a good way to display and select them. Multiselect is cool, but since it‚Äôs keyword‚Äëonly there‚Äôs no way to add layers.

**Idea:** swap to a text box: ‚ÄúI wore a blue shirt and my green pants.‚Äù The AI parses this, **searches** for existing pieces and returns their IDs. If ‚Äúblue shirt‚Äù exists, return it. If ‚Äúgreen pants‚Äù don‚Äôt, the AI **suggests** adding them as a new layer.

Maybe the AI doesn‚Äôt auto‚Äëadd‚Äîjust **suggests**. The client renders chips for ‚Äúblue shirt‚Äù and ‚Äúgreen pants.‚Äù The new/unknown one gets a subtle **confirmation checkbox**; once the user checks it, it becomes bold/confirmed and we insert it for real. First‚Äëtime users then get value immediately because they can add layers through this flow.

**Reason I haven‚Äôt implemented yet:** I‚Äôm mentally hung up on whether this belongs in **today‚Äôs log** view or as its **own view**.

### Generative UI Exploration

This connects to the broader UI philosophy. If the future is AI‚Äëfirst interfaces, how will that look? Today, AI generates Markdown (tables, bold, indentation). But **generative UI** is getting more interesting‚Äîlike what ChatGPT is starting to do for shopping experiences.

The Vercel AI SDK supports **generative UI**: [https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces](https://ai-sdk.dev/docs/ai-sdk-ui/generative-user-interfaces)

My current mobile design has **three panels** (chat, weather, wardrobe). It could be simplified to **just chat**, where the AI **invokes generative UI components** as functional calls‚Äîprebuilt React components that developers create, but the AI decides **when/how** to display them with the right data.

**Developer benefit:** you still create components/objects, but there‚Äôs no layout/positioning/spacing complexity relative to other components.

**Theoretical endpoint:** AI generates raw HTML/CSS/JS‚Äîor direct React components‚Äîcombining data and code generation beyond current generative UI.

### Image Input Integration

I‚Äôd love users to take a photo of their outfit, upload it, and have all the clothing pieces logged correctly.

---

## Final Conclusion

The entire scope of this project is only useful **until** LLMs‚Äô context windows effectively hold our whole lives. Until then, I think Layers is a meaningful way to capture personal context and turn it into better, personalized recommendations.

I‚Äôd love for people to try it and share feedback.

---

## Tech Stack

* **Frontend:** [Next.js](https://nextjs.org/) with [TypeScript](https://www.typescriptlang.org/)
* **Backend:** [Supabase](https://supabase.com/) + [Next.js API Routes](https://nextjs.org/docs/pages/building-your-application/routing/api-routes)
* **Styling:** [Tailwind CSS](https://tailwindcss.com/) + [shadcn/ui](https://ui.shadcn.com/)
* **AI:** [Vercel AI SDK](https://sdk.vercel.ai/) with [OpenAI API](https://openai.com/api/)
* **APIs:** [Google Maps API](https://developers.google.com/maps), [Visual Crossing Weather API](https://www.visualcrossing.com/weather-api/)
* **Hosting:** [Vercel](https://vercel.com/)

*This is a demo project exploring the intersection of AI and traditional user interfaces.*
